from interview.model import InterviewState
from interview.chroma_qa import get_similar_qa, save_qa_pair
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatGroq
import os
import json
import re
from dotenv import load_dotenv

# .env ë¡œë“œ
load_dotenv(dotenv_path="src/interview/.env")

# LLM ì´ˆê¸°í™”
llm = ChatGroq(
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    base_url="https://api.groq.com/openai/v1",
    model="llama3-8b-8192",
    temperature=0.7
)

# JSON ì‘ë‹µ ì•ˆì „ íŒŒì‹±
def safe_parse_json_from_llm(content: str) -> dict:
    print("ğŸ“¨ [LLM ì‘ë‹µ ì›ë¬¸ - ë‹¤ì‹œ í™•ì¸]:", content)
    cleaned = content.strip().replace("```json", "").replace("```", "").strip()

    try:
        result = json.loads(cleaned)
        print("âœ… JSON íŒŒì‹± ì„±ê³µ:", result)
        return result
    except json.JSONDecodeError as e:
        print(f"[!] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        match = re.search(r'(\{(?:[^{}]|(?:\{[^{}]*\}))*\})', cleaned, re.DOTALL)
        if match:
            json_block = match.group(1)
            try:
                result = json.loads(json_block)
                print("âœ… 2ì°¨ íŒŒì‹± ì„±ê³µ:", result)
                return result
            except json.JSONDecodeError as e2:
                print(f"[!] 2ì°¨ íŒŒì‹± ì‹¤íŒ¨: {e2}")

    print("[!] Fallback ì‚¬ìš© (ë¹ˆ ë”•ì…”ë„ˆë¦¬)")
    return {}

# í”„ë¡¬í”„íŠ¸ ì •ì˜
analysis_prompt = ChatPromptTemplate.from_messages([
    ("system", 
    """
ë„ˆëŠ” ë©´ì ‘ê´€ AIì•¼. ì•„ë˜ ì‚¬ìš©ìì˜ ë‹µë³€ì„ í‰ê°€í•œ ë’¤, ì•„ë˜ JSON í˜•ì‹ë§Œ ì‘ë‹µí•´.  
**ì ˆëŒ€ ì„¤ëª…í•˜ê±°ë‚˜ JSON ì™¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•˜ì§€ ë§ˆ!**

ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ë”°ë¼:

```json
{
  "good": "ê¸ì • í”¼ë“œë°± í•œ ì¤„",
  "bad": "ë¶€ì • í”¼ë“œë°± í•œ ì¤„",
  "score": ì •ìˆ˜(0~100)
}

""")
])

next_question_prompt = ChatPromptTemplate.from_messages([
    ("system", """
ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì§€ì›ìì˜ ì´ì „ ë‹µë³€ê³¼ ìê¸°ì†Œê°œì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¼¬ë¦¬ì§ˆë¬¸ì„ 1ê°œ ìƒì„±í•˜ì„¸ìš”.
í˜•ì‹ì€ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë¬¸ì¥ í•˜ë‚˜ë¡œ ì¶œë ¥í•˜ê³ , ì§ˆë¬¸ í˜•ì‹ìœ¼ë¡œ ëë‚´ì„¸ìš”.
ì˜ˆ: "ê·¸ ê²½í—˜ì—ì„œ ê°€ì¥ í˜ë“¤ì—ˆë˜ ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
"""),
    ("human", "ë‹µë³€: {answer}\nìê¸°ì†Œê°œì„œ: {resume}")
])

first_question_prompt = ChatPromptTemplate.from_messages([
    ("system", """
ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ì•„ë˜ ì§€ì›ìì˜ ìê¸°ì†Œê°œì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ
ë©´ì ‘ì—ì„œ ì‹œì‘í•  ì²« ë²ˆì§¸ ì§ˆë¬¸ì„ ìì—°ìŠ¤ëŸ½ê²Œ ìƒì„±í•˜ì„¸ìš”.

- ë„ˆë¬´ ê´‘ë²”ìœ„í•˜ì§€ ì•Šê²Œ, í•œ ë¬¸ì¥ìœ¼ë¡œ ì§ˆë¬¸ í˜•íƒœë¡œ ëë‚´ì„¸ìš”.
- ì˜ˆì‹œ: "ìê¸°ì†Œê°œì„œì— ë‚˜ì˜¨ í”„ë¡œì íŠ¸ ì¤‘ ê°€ì¥ ê¸°ì–µì— ë‚¨ëŠ” ê²½í—˜ì€ ë¬´ì—‡ì¸ê°€ìš”?"

ì§€ì›ìì˜ ìê¸°ì†Œê°œì„œ:
{resume}
""")
])

def answer_node(state: InterviewState) -> InterviewState:
    print("ğŸ—£ï¸ [answer_node] ë‹µë³€ ìˆ˜ì§‘ ì™„ë£Œ")
    if not state.answers:
        print("âš ï¸ [ê²½ê³ ] answersê°€ ë¹„ì–´ ìˆìŒ")
    state.step += 1
    return state

def analyze_node(state: InterviewState) -> InterviewState:
    print("ğŸ§  [analyze_node] ë‹µë³€ ë¶„ì„")
    answer = state.answers[-1] if state.answers else ""

    try:
        prompt = analysis_prompt.format_messages(answer=answer)
        response = llm.invoke(prompt)
        print("ğŸ§  [LLM ì‘ë‹µ]:", response.content)
        raw = response.content
        parsed = safe_parse_json_from_llm(raw)
    except Exception as e:
        print(f"ğŸ’¥ [ì˜ˆì™¸ ë°œìƒ]: {e}")
        parsed = {
            "good": "ë¶„ì„ ì‹¤íŒ¨",
            "bad": "ë¶„ì„ ì‹¤íŒ¨",
            "score": 0,
            "feedback": "ë¶„ì„ ì‹¤íŒ¨"
        }

    state.last_analysis = parsed
    state.seq += 1
    state.step += 1
    return state

def next_question_node(state: InterviewState) -> InterviewState:
    print("â¡ï¸ [next_question_node] ë‹¤ìŒ ì§ˆë¬¸ ìƒì„±")

    if len(state.questions) >= 3:
        state.is_finished = True
        return state

    answer = state.answers[-1] if state.answers else ""

    if not answer or answer.strip() == "":
        followup_q = "ì´ì „ì— ë§ì”€í•˜ì‹  ë‚´ìš©ì„ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì‹œê² ì–´ìš”?"
    else:
        try:
            prompt = next_question_prompt.format_messages(answer=answer, resume=state.text)
            response = llm.invoke(prompt)
            followup_q = response.content.strip()
            if not followup_q:
                raise ValueError("ë¹ˆ ì‘ë‹µ")
        except Exception as e:
            print(f"âŒ [ë‹¤ìŒ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨]: {e}")
            followup_q = "ì´ì „ì— ë§ì”€í•˜ì‹  ë‚´ìš©ì„ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì‹œê² ì–´ìš”?"

    # ğŸ”¹ ì§ˆë¬¸ ì €ì¥
    save_qa_pair(followup_q, answer)

    state.questions.append(followup_q)
    state.step += 1
    return state

def end_node(state: InterviewState) -> InterviewState:
    print("ğŸ [end_node] ë©´ì ‘ ì¢…ë£Œ")
    return state
